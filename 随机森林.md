随机森林在14年阿里巴巴天池大数据大赛以及kaggle比赛有非常好的表现。<br>
随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基础单元是决策树，而它的本质属于机器学习的一大分支————集成学习方法。
每棵决策树都是一个分类器，那么对于输入样本，N棵树会有N个分类结果，而随机森林集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出，这就是一种最简单的bagging思想。<br>

一.随机森林的相关基础知识
　　随机森林看起来是很好理解，但是要完全搞明白它的工作原理，需要很多机器学习方面相关的基础知识。在本文中，我们简单谈一下，而不逐一进行赘述，如果有同学不太了解相关的知识，可以参阅其他博友的一些相关博文或者文献。

　　1）信息、熵以及信息增益的概念

　　这三个基本概念是决策树的根本，是决策树利用特征来分类时，确定特征选取顺序的依据。理解了它们，决策树你也就了解了大概。

　　引用香农的话来说，信息是用来消除随机不确定性的东西。当然这句话虽然经典，但是还是很难去搞明白这种东西到底是个什么样，可能在不同的地方来说，指的东西又不一样。对于机器学习中的决策树而言，如果带分类的事物集合可以划分为多个类别当中，则某个类（xi）的信息可以定义如下:



　　I(x)用来表示随机变量的信息，p(xi)指是当xi发生时的概率。

　　熵是用来度量不确定性的，当熵越大，X=xi的不确定性越大，反之越小。对于机器学习中的分类问题而言，熵越大即这个类别的不确定性更大，反之越小。

　　信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好。

　　这方面的内容不再细述，感兴趣的同学可以看 信息&熵&信息增益 这篇博文。

　　2）决策树

　　决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。常见的决策树算法有C4.5、ID3和CART。

　　3）集成学习　

　　集成学习通过建立几个模型组合的来解决单一预测问题。它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成单预测，因此优于任何一个单分类的做出预测。

　　随机森林是集成学习的一个子类，它依靠于决策树的投票选择来决定最后的分类结果。
